### EfficientNet 笔记

对于神经网络,网络的特征图个数,层数,输入分辨率这些都会对结果有影响.并且单独的去提升这些指标都会有对应的瓶颈.所以合适的做法就是把这些因素综合起来.

谷歌就直接使用神经网络架构去做参数搜索(体现一个财大气粗),去综合的提升这些指标来更好的做出一个网络基线.

#### 基本网络结构

![EfficientNet-B0](https://user-images.githubusercontent.com/28779173/199876569-d9498808-76e2-4f73-a2c7-0f2d15197e37.png)

#### 深度可分离卷积(DepWise Conv)

通常的卷积都是将每个卷积核与输入特征图的每一层去进行计算,得到输出特征图.这样的短板就是计算量大.那么为了缩减计算量,就提出了深度可分离卷积.

深度可分类卷积的的做法是将卷积核与输入特征图的每一层一对一的去计算.这样就大大缩减了计算量.但是这种做法的弊端也很明显,就是无法改变特征图的个数.所以如果想要改变特征图的个数话,可以在后面追加一个1×1的卷积来完成.这样在同样能改变特征图个数的前提下,计算量也减少了.

#### 注意力机制(SE模块)

因为特征图是多层的.所以可能每层对结果的贡献都是不同的,可能有的层的特征不是那么重要,有的层的特征对解结果的影响很大.因此需要一组权层级的重参数对特征图进行处理.

具体的做法就是对输入的特征图做一个全局的平均池化,来得到一个1×1×c的特征向量,然后经过sigmoid函数得到每层的权重值.最后将这组权重值和输入特征图按位相乘就得到了经过注意力机制之后的特征图.

![SE模块](https://user-images.githubusercontent.com/28779173/199904409-3202eced-1bb8-4b39-b2bc-33ee04a46ae1.png)

#### 整体的MBConv结构

![MBConv](https://user-images.githubusercontent.com/28779173/199904699-fc21be11-139e-4db6-8a44-308462d52bfb.png)


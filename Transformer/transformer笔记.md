### Transformer笔记

#### 与传统的rnn对比

传统rnn的问题

* 传统的rnn网络是通过固定的词向量语料库来训练一个模型,那么模型训练好了之后,每个输入的token(将每个词转换成词向量,这个向量就叫token)就可以说是定死了的,即每个词在这个模型中他的特征就定死了.但是在实际语言环境中缺不是这样,相同的一个词,在不同的语境下,表示的意义可以是完全不一样的.所以传统的rnn网络做出的模型,效果不是很理想.
* 传统的rnn网络是一个串行的网络,串行就意味着模型的训练周期是很长的,并且由于串行的时间长,那么也不能进行多层的堆叠,因为这样周期就更久了

但是transformer解决了这些问题

* transformer使用了attention机制,对于每个词,transformer并没有用一个固定的token来表示它,而是根据上下文去产生一个综合的token去表示,这样就避免了传统rnn不能适应各种语境的问题
* transformer的self-attention是并行计算的,输入和输出都是相同的,都是同时被计算出来的,所以在时间上是大大加快的

#### self-attention机制

因为有语境这个条件了,那么一句话中的每个词对应的token现在不能单独考虑这个词本身了,每个token的组成都需要结合上下文的的各个词.

假设现在有一段话:$我吃西瓜$,通过语料表将它转换成token之后是: $[A,B,C,D]$ ,在传统的rnn中所有的token就这样定死了,这样就丢掉了语境.但是在transformer里,会将这个token进行进一步的加工(encoder): $A^\prime=w_1A+w_2B+w_3C+w_4D$ ,得到新的token: $[A^\prime,B^\prime,C^\prime,D^\prime]$  ,这个新的token就具有了上下文的信息.现在的问题就变成了这个 $w$ ,怎么得到?

在transformer中,我们设置三个辅助向量Q(query),K(keys),V(value)来做这个事,Q向量表示查询向量,是一个token向每个token去查询关联时用到,K向量表示应答向量,是当token被查询时,来做回应时用到,V向量表示真实特征向量,是每个token的真实特征值.那么在计算 $w_2$的时候,我们就可以用向量 $q_A$与向量 $k_B$做内积,因为内积是可以算出两个向量的相关程度的,这个时候计算出的内积值就是A和B的关联程度了,依次类推,我们可以计算A与A自身,与B,与C,与D的关联程度.又因为内积的值很可能是大于1的数值,我们就需要把这些值做成权重值,这个时候可以用softmax函数来进行转换得到对应的的 $[w_1,w_2,w_3,w_4]$,然后我们用对应的权重值乘上对应token的各自真实值V,再相加,就得到了转换后的具有上下文信息的新token:
$$
\vec{Z}=softmax({{\vec Q \cdot \vec K}\over \sqrt{d_k}})\cdot \vec{V}
$$
这里的 $d_k$表示对向量的维度,因为Q,K,V是我们自己定的,这里是为了避免因为向量的维度大小而对结果造成很大的影响,取根号是实验得出的结果

#### muti-headed机制(多头机制)

我们可以使用多次自注意力机制,得到多个Z向量,然后用一个FC来做降维,得到我们需要大小的结果,类似于卷积核的概念,用多个卷积核来提特征,最后综合到一起.

实际使用过程中,假设我们的token是512维的,我们需要做8头的话,我们可以每个头做64维,最后把8头拼在一起,最后就用一个1*1的FC去做综合

### Transformer笔记

#### 与传统的rnn对比

传统rnn的问题

* 传统的rnn网络是通过固定的词向量语料库来训练一个模型,那么模型训练好了之后,每个输入的token(将每个词转换成词向量,这个向量就叫token)就可以说是定死了的,即每个词在这个模型中他的特征就定死了.但是在实际语言环境中缺不是这样,相同的一个词,在不同的语境下,表示的意义可以是完全不一样的.所以传统的rnn网络做出的模型,效果不是很理想.
* 传统的rnn网络是一个串行的网络,串行就意味着模型的训练周期是很长的,并且由于串行的时间长,那么也不能进行多层的堆叠,因为这样周期就更久了

但是transformer解决了这些问题

* transformer使用了attention机制,对于每个词,transformer并没有用一个固定的token来表示它,而是根据上下文去产生一个综合的token去表示,这样就避免了传统rnn不能适应各种语境的问题
* transformer的self-attention是并行计算的,输入和输出都是相同的,都是同时被计算出来的,所以在时间上是大大加快的

#### self-attention机制

因为有语境这个条件了,那么一句话中的每个词对应的token现在不能单独考虑这个词本身了,每个token的组成都需要结合上下文的的各个词.

假设现在有一段话:$我吃西瓜$,通过语料表将它转换成token之后是: $[A,B,C,D]$ ,在传统的rnn中所有的token就这样定死了,这样就丢掉了语境.但是在transformer里,会将这个token进行进一步的加工(encoder): $A^\prime=w_1A+w_2B+w_3C+w_4D$ ,得到新的token: $[A^\prime,B^\prime,C^\prime,D^\prime]$  ,这个新的token就具有了上下文的信息.现在的问题就变成了这个 $w$ ,怎么得到?

在transformer中,我们设置三个辅助向量Q(query),K(keys),V(value)来做这个事,Q向量表示查询向量,是一个token向每个token去查询关联时用到,K向量表示应答向量,是当token被查询时,来做回应时用到,V向量表示真实特征向量,是每个token的真实特征值.


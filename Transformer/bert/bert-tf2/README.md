### bert说明

原始代码没法在TensorFlow2上跑,要大改,要在TensorFlow2上跑的话只能自己重写,原始代码只能看看前向传播,没法训练

bert属于是nlp里的一个通用的解决方案了,基本套用了标准的transformer架构,适用于各种下游任务.

#### bert模型的训练方式

bert的训练并不需要各种标签,只需要语料库就能进行了

* 预测屏蔽词

  直接将语料库中15%的词(中文可以是字)进行mask,然后让模型去预测被mask的字来进行训练.

* 预测两句话是否是上下文

  需要额外加一个cls向量来表示分类的结果,看两句话是否应该连接在一起.

#### 分词算法

分词的三种级别:

* ##### word

  直接用空格等标点符号这种自然语言里天然的分隔符来做,分出一个个自然的语言单元.是一种最简单的方式.
  
  缺点就是,由于长尾效应语料表可能超大,内存可能顶不住.
  
* ##### char

  用字符做分词,一般字符量都不大,所以语料表的大小没问题.但是又因为语料表又太小,导致每个字符去做embedding的时候,每个向量需要表达更多的语义,这样学习又不容易了.这个问题貌似比较针对英文,中文里按字分和按词分好像都还行,甚至按字分比按词要好点.可能是数据稀疏,OOV问题,过拟合这种导致的.

* ##### subword

  这种分词方式就介于字符和词之间,对常用词做完整的保留,生僻词就拆开成子词来共享token空间,例如External拆成es和ternal.这种方式同时兼容了语料库的大小和每个词向量语义的独立性

#### WordPiece分词算法

WordPiece的原理和BPE基本类似.

首先使用一个预分词器,根据空格,或者其他什么规则做一个预分词.并统计每个词出现的词频.

然后初始化词表,初始化的词表里包含组成每个词的字符.再根据预料表的字符串去做2-gram,3-gram..的组合,从这些组合里选出最大化训练数据似然的组合放到预料表里,例如:我们要把字符串A和字符串B的组合放到语料表里,那么 $P(AB) \over {P(A)P(B)}$ 应该是有最大值.然后不断重复到需要的语料库大小.


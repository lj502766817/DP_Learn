### bert说明

bert属于是nlp里的一个通用的解决方案了,基本套用了标准的transformer架构,适用于各种下游任务.

#### bert模型的训练方式

bert的训练并不需要各种标签,只需要语料库就能进行了

* 预测屏蔽词

  直接将语料库中15%的词(中文可以是字)进行mask,然后让模型去预测被mask的字来进行训练.

* 预测两句话是否是上下文

  需要额外加一个cls向量来表示分类的结果,看两句话是否应该连接在一起.

### Transformer笔记

#### 与传统的rnn对比

传统rnn的问题

* 传统的rnn网络是通过固定的词向量语料库来训练一个模型,那么模型训练好了之后,每个输入的token(将每个词转换成词向量,这个向量就叫token)就可以说是定死了的,即每个词在这个模型中他的特征就定死了.但是在实际语言环境中缺不是这样,相同的一个词,在不同的语境下,表示的意义可以是完全不一样的.所以传统的rnn网络做出的模型,效果不是很理想.
* 传统的rnn网络是一个串行的网络,串行就意味着模型的训练周期是很长的,并且由于串行的时间长,那么也不能进行多层的堆叠,因为这样周期就更久了

但是transformer解决了这些问题

* transformer使用了attention机制,对于每个词,transformer并没有用一个固定的token来表示它,而是根据上下文去产生一个综合的token去表示,这样就避免了传统rnn不能适应各种语境的问题
* transformer的self-attention是并行计算的,输入和输出都是相同的,都是同时被计算出来的,所以在时间上是大大加快的

#### self-attention机制

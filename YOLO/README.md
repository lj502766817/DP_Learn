## YOLO系列笔记

深度学习的经典检测方法,通常分为两类:

* one stage:

  yolo系列

  通常是使用一个特征提取网络,然后根据网络提取的特征,直接预测目标的位置.

  one stage的优势是速度很快,适合一些实时的检测任务.缺点就是相对来说检测的精度不是那么高.

* two stage:

  Faster-Rcnn,Mask-Rcnn系列

  与one stage不同的是,two stage是先用一个网络提取目标的候选框,然后根据候选区,再用CNN网络来分类.

  因为涉及到了两个阶段,因此,two stage的FPS通常较低,但是相对来说检测的精度就上来了.

一些名词解释:

* **TP**:检索到正类并判定为了正类
* **FP**:检索到负类并判定为正类
* **FN**:检索到正类并判定成负类
* **TN**:检索到负类并判定成负类
* **精度(precision)**: $TP\over{TP+FP}$ ,即在所有判定为正类结果中,真正为正类的比例.
* **召回率(recall)**: $TP\over{TP+FN}$ ,即在所有的正类中,被检索到的比例.
  *(精度和召回率通常是互斥的)*
* **IOU(Intersection over Union)**: 检测框区域与真实框区域的交集大小比上检测框区域与真实框区域的并集大小
* **mAP**:AP指的就是精度和召回率,mAP就是对精度和召回率的平均结果.计算方式为:针对检测的IOU,我们可以设置不同的置信度,而这些置信度产生的结果又是一对对精度和召回率的元组.然后就可以画出精度和召回率的PR曲线图,这个曲线图下的面积就是mAP值.

### YOLO-V1

YOLO-V1版本提取特征的网络结构很简单,就是一个CNN的网络没什么说的.

V1版本的重点在于最后的输出层.经过前面的CNN网络去提取特征,得到一个(1470,1)的特征向量.然后把这个特征向量reshape成一个(7,7,30)的特征图,这个特征图就是重点.

前面的7\*7表示,最后的结果是把原始图像切割成了7\*7的网格.然后由于V1版本是默认对每个检测目标都有两个候选框,因此后面的30,先拆分成5+5+20.这里面的5表示在一个候选框中的中心点(x,y),长宽(w,h),以及区域的置信度c的值,所以有两个5,然后做的是20分类,因此后面的20就表示20分类的概率值.

##### 损失函数

V1版本最关键的点就是定义了YOLO目标检测的损失函数

YOLO的损失函数有三个部分:

![YOLO损失函数](https://user-images.githubusercontent.com/28779173/196934339-d9177969-73a5-41a3-bc7e-c2ac8885d8af.jpg)


* 第一个是位置误差部分:就是每个单元格以及每个候选框预测值的中心点,长宽与标签值的差异

* 第二个是置信度误差部分:就是预测值与标签值置信度的差异,不过需要注意的一点是,在置信度损失函数部分,因为在一张图中,检测目标以外的区域可能是有很大的,那么为了避免非检测目标对检测目标损失函数造成太大的干扰,因此需要在对非检测目标区域的置信度损失函数加一个 $\lambda$ 的权重控制

* 第三个是分类误差部分,就是一个交叉熵损失函数就行了

这三个部分合起来就是YOLO的损失函数.神经网络通过第一部分来修正每次学习的位置误差,通过第二分部来修正置信度误差,第三部分来修正分类误差

##### 非极大值抑制
因为对一个目标的检测,有可能有很多到的中心点以及候选框,那么我们需要选出一个最优的候选框.这个时候需要进行非极大值抑制,只选IOU最大的那个

##### 总结
YOLO-V1版本的属于YOLO系列的基础
它的优势在于:整个网络很简单,这样就使得检测速度很快.但是这样也带来了弊端:每个Cell只能检测到一个类别,如果有重叠就没有办法了,并且小物体的检测效果一般,长宽比的选择也很少.

### YOLO-V2

鉴于YOLO-V1版本的缺陷.YOLO-V2在网络结构,先验候选框等方面进行了优化.

##### 网络结构改善

* 去掉Dropout层,改成Conv层和BN层的结合,一层Conv层连一层BN层(现在几乎全都这么做了),归一化后,收敛相对容易.最后效果有2%mAP的提升

* 训练时除开原本V1版本那样用224\*224的分辨率之外,还额外使用了10次448\*448的分辨率进行微调,增强了模型的泛化能力,最后效果有4%mAP的提升

* 去掉了FC层,V1版本再最后输出的时候,先把特征图做成特征向量,然后经过FC,然后又转回特征图.这种操作很繁琐,而且FC层的参数很多.V2版本使用了5次降采样Maxpool和1\*1的Conv来替代.相比较来说,这样参数会小很多,那么模型的速度也会更快

  

##### 先验候选框

V1的候选框是默认的2种,而且大小是常规预置的.这种可能并不能适合自己任务的数据集.因此在V2的版本里使用K-means算法来提候选框.将样本标签按照相互IOU的值进行聚类,然后可以选出多个候选框.最后结果上,虽然对比V1版本mAP值虽然没上升多少,但是recall从81%上升到了88%.

在K-means算法里使用的距离并不是通常的欧式距离.而是,使用了IOU来进行计算,选出5种候选框:
$$
d(box,centroides) = 1-IOU(box,centroids)
$$

##### 检测框改善

在V1中的候选框是直接使用的预测便宜量.但是,在模型刚开始训练的时候,因为是随机选取的原因,偏移值可能很大,这样就会导致模型的收敛不好,模型不稳定.

在V2里就使用了相对grid cell的偏移量:

$$
\begin{aligned}
& b_x = \sigma(t_x)+c_x \\
& b_y = \sigma(t_y)+c_y \\
& b_w = p_we^{t_w} \\
& b_h = p_he^{t_h}
\end{aligned}
$$

使用 sigmoid函数就可以限制预测的中心点不管怎么便宜,都会在grid cell里面,不会偏移出去,模型的收敛就会好一些.

##### 感受野的问题

因为最后用来预测的特征图是经过多次卷积之后的结果.因此最后一层的感受野就太大了,这样就有可能丢失掉小目标.因此就需要融合前面感受野不是那么大的特征.

例如,如果最后一层的特征图是13\*13\*1024的,那么就把它前面层的特征图做拆分,拆成4\*13\*13\*512的,然后做个叠加拼接,最后的结果就是13\*13\*3072的特征图.

##### 使用多尺度

因为在V2中,把所有的FC都去掉了,因此对输入数据的格式就没那么多限制了,就可以在一定的迭代之后,改变输入图像的大小.

V2中最小的格式可以是320\*320,最大的为608\*608.

### YOLO_V3

V3版本最大的改进就是在网络结构上做更好的优化,特征做的更加细致,更适合小目标检测.而且先验框更加丰富,更适合多种规格的目标检测.并且将最后分类的softmax函数做改进,使得网络可以进行多标签的任务.

##### 网络优化

* 多scale设计

  在V2中,因为感受野的问题,为了避免丢失小目标,就将前面层的特征图和最后一层做了融合.但实际上,这样融合可能有效果,也有可能使感受野大的那一层被感受野小的那一层影响,特征没那么好了.于是在V3中,对不同规格的大小的目标,就干脆使用同scale的特征图去进行预测.

  在V3中设计了3个scale:

  - 52\*52\*c的,小感受野,用来预测小目标.
  - 26\*26\*c的,中等感受野,用来预测中等目标.
  - 13\*13\*c的,大感受野,用来预测大目标.

  但是有个问题就是,小感受野的时候,网络的特征提取的不是很好,这样会导致预测的结果不理想.于是在V3里,对小目标的预测不仅仅使用小感受野的特征图.而是,先将大感受野的特征图做一个上采样13\*13的特征图转成26\*26的特征图,然后和原本26\*26的特征图叠加,来做预测,就相当于特征提取的好的一层来带着特征提取的一般的一层.

* 残差连接

  多层堆叠的话,现在基本就无脑用了

* softmax层替换

  因为softmax函数只能预测一个标签.因此,针对多标签的情况下,就像逻辑回归那样,使用sigmoid函数来预测每一个类别是或者不是,就把一个多分类的任务转换成多个二分类任务.

##### 先验框的改进

因为V3的网络是在多个尺度上进行预测的.所以相对于V2中的5个先验框,V3里使用了9种先验框.在coco数据集上,先聚类找到9种尺寸,然后分类:

* 在13\*13的特征图上,使用(116\*90),(156\*198),(373\*326)
* 在26\*26的特征图上,使用(30\*61),(62\*45),(59\*119)
* 在52\*52的特征图上,使用(10\*13),(16\*30),(33\*23)

##### V3整体结构

![YOLO-V3结构图](https://user-images.githubusercontent.com/28779173/197393933-fa8dc500-d6cf-4942-8d24-a8ee3e0e8299.jpg)

在V3中,网络结构里,去掉了池化层和全连接层,全部用卷积层替代了,下采样的操作通过设置卷积层的stride为2来实现.


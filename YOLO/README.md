## YOLO系列笔记

深度学习的经典检测方法,通常分为两类:

* one stage:

  yolo系列

  通常是使用一个特征提取网络,然后根据网络提取的特征,直接预测目标的位置.

  one stage的优势是速度很快,适合一些实时的检测任务.缺点就是相对来说检测的精度不是那么高.

* two stage:

  Faster-Rcnn,Mask-Rcnn系列

  与one stage不同的是,two stage是先用一个网络提取目标的候选框,然后根据候选区,再用CNN网络来分类.

  因为涉及到了两个阶段,因此,two stage的FPS通常较低,但是相对来说检测的精度就上来了.

一些名词解释:

* **TP**:检索到正类并判定为了正类
* **FP**:检索到负类并判定为正类
* **FN**:检索到正类并判定成负类
* **TN**:检索到负类并判定成负类
* **精度(precision)**: $TP\over{TP+FP}$ ,即在所有判定为正类结果中,真正为正类的比例.
* **召回率(recall)**: $TP\over{TP+FN}$ ,即在所有的正类中,被检索到的比例.
  *(精度和召回率通常是互斥的)*
* **IOU(Intersection over Union)**: 检测框区域与真实框区域的交集大小比上检测框区域与真实框区域的并集大小
* **mAP**:AP指的就是精度和召回率,mAP就是对精度和召回率的平均结果.计算方式为:针对检测的IOU,我们可以设置不同的置信度,而这些置信度产生的结果又是一对对精度和召回率的元组.然后就可以画出精度和召回率的PR曲线图,这个曲线图下的面积就是mAP值.

### YOLO-V1

原论文地址:https://arxiv.org/pdf/1506.02640v5.pdf

YOLO-V1版本提取特征的网络结构很简单,就是一个CNN的网络没什么说的.

V1版本的重点在于最后的输出层.经过前面的CNN网络去提取特征,得到一个(1470,1)的特征向量.然后把这个特征向量reshape成一个(7,7,30)的特征图,这个特征图就是重点.

前面的7\*7表示,最后的结果是把原始图像切割成了7\*7的网格.然后由于V1版本是默认对每个检测目标都有两个候选框,因此后面的30,先拆分成5+5+20.这里面的5表示在一个候选框中的中心点(x,y),长宽(w,h),以及区域的置信度c的值,所以有两个5,然后做的是20分类,因此后面的20就表示20分类的概率值.

##### 损失函数

V1版本最关键的点就是定义了YOLO目标检测的损失函数

YOLO的损失函数有三个部分:

![YOLO损失函数](https://user-images.githubusercontent.com/28779173/196934339-d9177969-73a5-41a3-bc7e-c2ac8885d8af.jpg)


* 第一个是位置误差部分:就是每个单元格以及每个候选框预测值的中心点,长宽与标签值的差异

* 第二个是置信度误差部分:就是预测值与标签值置信度的差异,不过需要注意的一点是,在置信度损失函数部分,因为在一张图中,检测目标以外的区域可能是有很大的,那么为了避免非检测目标对检测目标损失函数造成太大的干扰,因此需要在对非检测目标区域的置信度损失函数加一个 $\lambda$ 的权重控制

* 第三个是分类误差部分,就是一个交叉熵损失函数就行了

这三个部分合起来就是YOLO的损失函数.神经网络通过第一部分来修正每次学习的位置误差,通过第二分部来修正置信度误差,第三部分来修正分类误差

##### 非极大值抑制
因为对一个目标的检测,有可能有很多到的中心点以及候选框,那么我们需要选出一个最优的候选框.这个时候需要进行非极大值抑制,只选IOU最大的那个

##### 总结
YOLO-V1版本的属于YOLO系列的基础
它的优势在于:整个网络很简单,这样就使得检测速度很快.但是这样也带来了弊端:每个Cell只能检测到一个类别,如果有重叠就没有办法了,并且小物体的检测效果一般,长宽比的选择也很少.

### YOLO-V2

原论文地址:https://arxiv.org/pdf/1612.08242v1.pdf

鉴于YOLO-V1版本的缺陷.YOLO-V2在网络结构,先验候选框等方面进行了优化.

##### 网络结构改善

* 去掉Dropout层,改成Conv层和BN层的结合,一层Conv层连一层BN层(现在几乎全都这么做了),归一化后,收敛相对容易.最后效果有2%mAP的提升

* 训练时除开原本V1版本那样用224\*224的分辨率之外,还额外使用了10次448\*448的分辨率进行微调,增强了模型的泛化能力,最后效果有4%mAP的提升

* 去掉了FC层,V1版本再最后输出的时候,先把特征图做成特征向量,然后经过FC,然后又转回特征图.这种操作很繁琐,而且FC层的参数很多.V2版本使用了5次降采样Maxpool和1\*1的Conv来替代.相比较来说,这样参数会小很多,那么模型的速度也会更快

  

##### 先验候选框

V1的候选框是默认的2种,而且大小是常规预置的.这种可能并不能适合自己任务的数据集.因此在V2的版本里使用K-means算法来提候选框.将样本标签按照相互IOU的值进行聚类,然后可以选出多个候选框.最后结果上,虽然对比V1版本mAP值虽然没上升多少,但是recall从81%上升到了88%.

在K-means算法里使用的距离并不是通常的欧式距离.而是,使用了IOU来进行计算,选出5种候选框:
$$
d(box,centroides) = 1-IOU(box,centroids)
$$

##### 检测框改善

在V1中的候选框是直接使用的预测便宜量.但是,在模型刚开始训练的时候,因为是随机选取的原因,偏移值可能很大,这样就会导致模型的收敛不好,模型不稳定.

在V2里就使用了相对grid cell的偏移量:

$$
\begin{aligned}
& b_x = \sigma(t_x)+c_x \\
& b_y = \sigma(t_y)+c_y \\
& b_w = p_we^{t_w} \\
& b_h = p_he^{t_h}
\end{aligned}
$$

使用 sigmoid函数就可以限制预测的中心点不管怎么便宜,都会在grid cell里面,不会偏移出去,模型的收敛就会好一些.

##### 感受野的问题

因为最后用来预测的特征图是经过多次卷积之后的结果.因此最后一层的感受野就太大了,这样就有可能丢失掉小目标.因此就需要融合前面感受野不是那么大的特征.

例如,如果最后一层的特征图是13\*13\*1024的,那么就把它前面层的特征图做拆分,拆成4\*13\*13\*512的,然后做个叠加拼接,最后的结果就是13\*13\*3072的特征图.

##### 使用多尺度

因为在V2中,把所有的FC都去掉了,因此对输入数据的格式就没那么多限制了,就可以在一定的迭代之后,改变输入图像的大小.

V2中最小的格式可以是320\*320,最大的为608\*608.

### YOLO-V3

原论文地址:https://arxiv.org/pdf/1804.02767v1.pdf

V3版本最大的改进就是在网络结构上做更好的优化,特征做的更加细致,更适合小目标检测.而且先验框更加丰富,更适合多种规格的目标检测.并且将最后分类的softmax函数做改进,使得网络可以进行多标签的任务.

##### 网络优化

* 多scale设计

  在V2中,因为感受野的问题,为了避免丢失小目标,就将前面层的特征图和最后一层做了融合.但实际上,这样融合可能有效果,也有可能使感受野大的那一层被感受野小的那一层影响,特征没那么好了.于是在V3中,对不同规格的大小的目标,就干脆使用同scale的特征图去进行预测.

  在V3中设计了3个scale:

  - 52\*52\*c的,小感受野,用来预测小目标.
  - 26\*26\*c的,中等感受野,用来预测中等目标.
  - 13\*13\*c的,大感受野,用来预测大目标.

  但是有个问题就是,小感受野的时候,网络的特征提取的不是很好,这样会导致预测的结果不理想.于是在V3里,对小目标的预测不仅仅使用小感受野的特征图.而是,先将大感受野的特征图做一个上采样13\*13的特征图转成26\*26的特征图,然后和原本26\*26的特征图叠加,来做预测,就相当于特征提取的好的一层来带着特征提取的一般的一层.

* 残差连接

  多层堆叠的话,现在基本就无脑用了

* softmax层替换

  因为softmax函数只能预测一个标签.因此,针对多标签的情况下,就像逻辑回归那样,使用sigmoid函数来预测每一个类别是或者不是,就把一个多分类的任务转换成多个二分类任务.

##### 先验框的改进

因为V3的网络是在多个尺度上进行预测的.所以相对于V2中的5个先验框,V3里使用了9种先验框.在coco数据集上,先聚类找到9种尺寸,然后分类:

* 在13\*13的特征图上,使用(116\*90),(156\*198),(373\*326)
* 在26\*26的特征图上,使用(30\*61),(62\*45),(59\*119)
* 在52\*52的特征图上,使用(10\*13),(16\*30),(33\*23)

##### V3整体结构

![YOLO-V3结构图](https://user-images.githubusercontent.com/28779173/197393933-fa8dc500-d6cf-4942-8d24-a8ee3e0e8299.jpg)

在V3中,网络结构里,去掉了池化层和全连接层,全部用卷积层替代了,下采样的操作通过设置卷积层的stride为2来实现.

### YOLO-V4

原论文地址:https://arxiv.org/pdf/2004.10934v1.pdf

V4版本的改进属于是把当时一些优秀的方法都用上去了.改进的重点是两个层面:

* 数据层面(Bag of freebies):增加了训练的成本,但是模型的精度上去了,而且不影响推理的速度
* 网络结构层面(Bag of specials):稍微增加了推理的代价,但是能增加精度

并且V4版本最重要的点是在单GPU的情况下也能训练的不错

##### 图像数据增强

除开基本常规的图像数据增强策略:亮度调整,对比度,色调,随机缩放,剪切,旋转,翻卷.在V4里还使用了其他的数据增强策略

* Mosaic data augmentation

  论文的作者将原始图像,MixUp图像,CutOut图像,CutMix图像都做了检测任务后,将结果进行对比,发现CutMix对结果有一些增强.于是在V4中,加上了CutMix来做图像增强
  CutMix就是将原本需要拿来做训练的4张图像,拼到一起,然后合成的图像里面该标注还是正常标注.再用合成图像来做训练.这样在batch不变的情况下,样本更加丰富了.

* Random Erase

  用一些随机值或者训练集的平均像素值来替换训练图像的一些区域

* Hide and Seek

  根据概率随机的在图像上打一些补丁来掩盖

* Self-adversarial-training(SAT)

  根据一定的权重,给图像加一些噪声像素在里面.

* DropBlock

  在基础的神经网络中,我们使用了dropout按点杀死神经元,来增强网络的泛华能力.但是在卷积神经网络中这样还不够,V4中作者使用了dropblock,按照区域来做,对网络的泛华能力可以做的更好

* Class label smoothing

  对于分类的标签,V4进行了平滑的处理,并不是[0,1]这么绝对的标签.而是使用了 $[0,1]*(1-0.1)+{0.1\over2}=[0.05,0.95]$ 这样的平滑处理.这样使模型的学习不会那么绝对,模型的分类效果在簇内更加的紧密,簇间更加的明显.

##### 损失函数的改进

V3版本的损失函数是用IOU来做的.但是单纯使用IOU来做的话是有弊端的:预测区域如果和真实区域完全没交集的话,那么这时候的IOU就是0了,这时候损失函数是IOU的话就出现了梯度消失的情况,模型就无法更新了.还有就是在IOU是某个值的情况下,预测区域和真实区域是有多种重叠的可能性的,这个时候IOU是没法表示当前的状况的.

于是在V4中对损失函数做了升级:

* 一阶段升级:GIOU
  
  $$
  \mathcal{L}_{GIoU}=1-IoU+{|C-B\cap B^{gt}|\over|C|}
  $$
  
  这里引入了一个最小包围框C(C把预测框和真实框包裹在内).这样,在不重叠的情况下,就能让预测框不断的向真实框去靠近.但是这个GIOU也有个弊端,就是如果一个框在另一个框里面的话,就没作用了,此时的C就失去意义了.

* 二阶段升级:DIOU
  
  $$
  \mathcal{L}_{DIoU}=1-IoU+{\rho^2(b,b^{gt})\over c^2}
  $$
  
  这里引入了预测框与真实框的中心点欧氏距离,然后与C的对角线长度的比值来做.这样直接优化距离,速度更快,并且可以解决GIOU的缺陷.

* 三阶段升级:CIOU

  $$
  \begin{aligned}
  & \mathcal{L}_{CIoU}=1-IoU+{\rho^2(b,b^{gt})\over c^2}+\alpha v \\
  & v= {4\over\pi^2}(arctan{w^{gt}\over h^{gt}}-arctan{w\over h}) \\
  & \alpha = {v\over(1-IoU)+v} \\
  \end{aligned}
  \tag{$\alpha$可以看做是权重值}
  $$
  
  最后作者把所有的集合因素都考虑进来:重叠面积,中心点距离,长宽比.

##### NMS改进

在V3里NMS是完全基于IOU去做的,但是在V4里使用的是DIOU-NMS,还加入了候选框中心点间的距离:

$$
s_i=
\begin{cases}
s_i,\quad IoU-\mathcal{R}_{DIoU}(M,B_i) < \epsilon \\
0,\quad IoU-\mathcal{R}_{DIoU}(M,B_i) \geq \epsilon
\end{cases} 
,
\mathcal{R}_{DIoU}(M,B_i)={\rho^2(b,b^{gt})\over c^2}
$$


##### SPPNet

参考SPP的空间金字塔的概念.在V4中也加入了SPP模块,将特征图经过多个maxpool后,这些特征图就拥有了不同感受野的特征,最后再concat再一起.这样显著提高了backbone的感受野.

##### CSPNet

在这个block中,作者将输入的特征图按照channel做切分,一部分走block过,另一部分直接concat到block的结果上.这样做的好处是在几乎不影响精度的情况下,大大提升了计算的速度.

##### CBAM

注意力机制,V4版本也加进来了,原始的CBAM是由两个部分组成的,CAM和SAM:

![CBAM](https://user-images.githubusercontent.com/28779173/198267551-0365b70f-19ac-4524-a1d7-2a35395de55b.jpg)

即在channel层面的注意力机制和在图像平面的注意力机制.
而在YOLO中做了简化,去掉了CAM,并且简化SAM.将原始CAM里通过channel维度maxpool和avgpool得到的特征图的操作直接换成用原始特征图来做.
这样的操作在引入注意力机制增加精度的同时,又不会带来太大的计算负担.

##### PAN

PAN的做法来自于FPN.
在V3版本中,用高层的特征图往底层去融合就是FPN的做法.但是特征融合不仅仅可以高层往底层融合,底层也可以往高层融合.因此就有了PAN,在高层往底层融合完成后,再从底层往高层融合.

##### Mish

新的激活函数,相对于relu来说,在负轴的部分不是完全归零,而是留了一点.在使用了Mish后检测的精度在top-1和top-5上都有一个点的提升

##### eliminate grid sensitivity

这个就是避免,中心点在cell边界的情况.因为偏移量是经过了sigmoid函数转换的,但是sigmoid函数在取0或1的时候需要很大的输入值,因此在sigmoid函数前加一个权重值,可以优化这种情况.

##### V4整体结构

![YOLO-V4网络结构](https://user-images.githubusercontent.com/28779173/198267238-6c547606-76e6-4995-8686-20244c0c8602.jpg)
